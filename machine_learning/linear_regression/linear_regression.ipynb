{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a9792a-f27f-4531-a174-022fae3f16aa",
   "metadata": {},
   "source": [
    "## Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa00dfd-e2f8-4a33-b18b-0ed83ed18942",
   "metadata": {},
   "source": [
    "A regressão linear é um dos modelos mais conhecidos no contexto do aprendizado de máquina, resumindo-se à adequação de um modelo linear a dados cujos preditores (features) são altamente correlacionados com o valor que se quer estimar (target). Ela pode ser implementada através do algoritmo de Gradiente Descendente Estocástico: um algoritmo iterativo de otimização dos parâmetros da função de regressão baseado na minimização da função de custo (ou de perda). Essa minimização é feita com o cálculo do gradiente negativo da função de custo em relação aos parâmetros da função de regressão (weights e bias).\n",
    "\n",
    "Com n features, o modelo será da forma:\n",
    "\n",
    "$\\hat{y} = w_{1}x_{1} + w_{2}x_{2} + ... + w_{m}x_{m} + b$\n",
    "\n",
    "Em que $\\hat{y}$ é a estimativa (ou predição) da label $y$, dados os valores das features $x_{1}, x_{2},...,x_{m}$, $(w_{1}, w_{2},...,w_{m})$ é o conjunto de pesos (weight) associado às features e $b$ é chamado de viés (bias).\n",
    "\n",
    "Vamos definir a perda (loss) associada ao modelo através da função de perda $L_{2}$, da mesma forma que foi feito no primeiro modelo. Assim:\n",
    "\n",
    "$loss = \\frac{1}{n}\\sum_{i = 1}^{n} (\\hat{y_{i}} - y_{i})^2$\n",
    "\n",
    "Como queremos reduzir a perda de nosso modelo ao máximo, devemos fazer os parâmetros variarem na direção do  negativo do gradiente de f (esse é o princípio do algoritmo de Gradiente Descendente Estocástico). \n",
    "\n",
    "Nesse contexto, chamemos $\\theta = (w_{1}, w_{2}, ... , w_{m}, b)$. Além disso, definimos o hiperparâmetro $\\alpha$ como a taxa de aprendizagem do modelo, de forma análoga ao modelo anterior.\n",
    "\n",
    "Assim, por meio do gradiente descendente estocástico, o novo valor de $\\theta$, $\\theta'$, será dado pela fórmula:\n",
    "\n",
    "$\\theta' = \\theta - \\alpha \\cdot \\nabla f(w_{1}, w_{2}, ... , w_{m}, b)$\n",
    "\n",
    "Onde $\\nabla f(w_{1}, w_{2}, ... , w_{m}, b) = (\\frac{\\partial f}{\\partial w_{1}}, \\frac{\\partial f}{\\partial w_{2}},..., \\frac{\\partial f}{\\partial w_{m}}, \\frac{\\partial f}{\\partial b})$ é o gradiente de f.\n",
    "\n",
    "Calculando as derivadas parciais:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i}$, para toda feature $x_{k}$.\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$\n",
    "\n",
    "Obtemos os novos valores do peso e do viés após uma iteração:\n",
    "\n",
    "$w_{k}' = w_{k} - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i}$, para toda feature $x_{k}$.\n",
    "\n",
    "$b' = b - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$\n",
    "\n",
    "Iremos modificar o modelo anterior e adicionar o termo de complexidade à função de custo, definindo-o pela fórmula de regularização $L_{2}$. Tal termo é dado por:\n",
    "\n",
    "$L_{2}$ regularization term = $ \\sum_{i = 1}^{m} w_{i}^{2}$\n",
    "\n",
    "Definimos $\\lambda$ como o coeficiente de regularização, um hiperparâmetro do modelo que multiplica a função de complexidade. Quanto maior $\\lambda$, maior o efeito de regularização na complexidade, isto é, mais penalizados os pesos muito grandes nas features são. Dessa forma, a nova função que se quer minimizar no modelo é dada por:\n",
    "\n",
    "$f(\\theta)$ = Loss + $\\lambda$ Complexity = $\\frac{1}{n} \\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})^{2} + \\lambda \\sum_{i = 1}^{m} w_{i}^{2}$\n",
    "\n",
    "cujas derivadas parciais são dadas por:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w_{k}} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i} + 2\\lambda w_{k}$, para toda feature $x_{k}$ com peso $w_{k}$.\n",
    "\n",
    "$\\frac{\\partial f}{\\partial b} = \\frac{2}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$\n",
    "\n",
    "Assim, obtemos os novos valores do peso e do viés após uma iteração:\n",
    "\n",
    "$w_{k}' = (1 - 2 \\alpha \\lambda)\\;w_{k} - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i}) \\cdot (x_{k})_{i}$, para toda feature $x_{k}$ com peso $w_{k}$.\n",
    "\n",
    "$b' = b - \\frac{2 \\alpha}{n}\\sum_{i = 1}^{n} (\\hat{y}_{i} - y_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e990f-30b3-4fcd-b66f-07554ad3e731",
   "metadata": {},
   "source": [
    "### Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1779d-a191-45e2-ae2e-1b7eb124d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.utils import *\n",
    "from linear_regressor import LinearRegressor\n",
    "from linear_regression_parameters import LinearRegressionParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37648c-187e-4c53-9224-fb56e9cf24e4",
   "metadata": {},
   "source": [
    "### Importando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a7b35-9396-4cc7-9fd3-83761b914f97",
   "metadata": {},
   "source": [
    "Utilizaremos o dataset \"Data for Admission in the University\", disponível no Kaggle.\n",
    "\n",
    "https://www.kaggle.com/datasets/akshaydattatraykhare/data-for-admission-in-the-university"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a2a55-c002-4d44-8908-e12a5cc75291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join('data', 'adm_data.csv')\n",
    "data = pd.read_csv(data_path)\n",
    "data.rename(columns = {'Chance of Admit ': 'Chance of Admit'}, inplace = True)\n",
    "data.rename(columns = {'LOR ': 'LOR'}, inplace = True)\n",
    "data.drop([\"Serial No.\"], axis = 1, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ca935-aada-4883-94ec-0c7373a43417",
   "metadata": {},
   "source": [
    "### Definindo os Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447e559-d126-4402-8edb-b62b686012c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = LinearRegressionParameters()\n",
    "parameters.initial_weights = np.zeros(len(data.columns) - 1)\n",
    "parameters.epochs = 50\n",
    "parameters.batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24608657-82db-46e8-a609-f6166a01a79f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac506c0-3c18-42ab-a0fa-e3c567705469",
   "metadata": {},
   "source": [
    "### Shuffle, Split e Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50114d-d01b-44b5-a02a-bda5ab66b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle_data(data, parameters.random_state)\n",
    "X = data.drop(columns = [\"Chance of Admit\"])\n",
    "y = data[\"Chance of Admit\"]\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, 0.15, 0.15)\n",
    "X_val = standard_scale(X_val, X_train)\n",
    "X_test = standard_scale(X_test, X_train)\n",
    "X_train = standard_scale(X_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7fcc88-282d-4a53-a9ad-d591e7c1b8ba",
   "metadata": {},
   "source": [
    "### Verificando as correlações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c7698-9a94-4827-a4e5-ae5b78d29082",
   "metadata": {},
   "source": [
    "Isso é feito nos dados de treino apenas, pois são neles que o modelo será baseado, evitando assim data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada9e24-f74c-4913-a7b2-c4ba83d2fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d148c-1bf4-4c79-bd43-b2607f0125fc",
   "metadata": {},
   "source": [
    "### Treinando um Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685d234-00d4-4fe8-ac63-de9169e38dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegressor(parameters)\n",
    "model.fit(X_train, y_train, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904128e0-660e-4f44-a95d-47526af38ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(model.get_train_loss())\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0437a25-5c27-4e4d-ac0c-0dba31e09d59",
   "metadata": {},
   "source": [
    "### Validação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df675cb9-4769-4ef4-ac4a-7a7732d61708",
   "metadata": {},
   "source": [
    "Escolhendo o melhor valor de lambda (hiperparâmetro da regularização), ou seja, aquele que gera o menor loss na validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae48ee8-e789-462a-adc4-6a8ca9c7ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "lambdas = [round(x, 2) for x in np.arange(0, 1.05, 0.05)]\n",
    "val_losses = []\n",
    "for lambda_reg in lambdas:\n",
    "    parameters = LinearRegressionParameters()\n",
    "    parameters.lambda_reg = lambda_reg\n",
    "    parameters.initial_weights = np.zeros(len(data.columns) - 1)\n",
    "    parameters.epochs = 50\n",
    "    parameters.batch_size = 20\n",
    "    model = LinearRegressor(parameters)\n",
    "    model.fit(X_train, y_train)\n",
    "    models[lambda_reg] = model\n",
    "    val_losses.append(loss(model, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58131ce8-0227-404e-a8fb-330a18a7c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(lambdas, val_losses)\n",
    "plt.suptitle(\"Loss na validação em função de lambda (regularização)\")\n",
    "plt.title(f\"O menor valor de função de custo ocorre para lambda = {lambdas[np.argmin(val_losses)]}\")\n",
    "plt.xlabel(\"Lambda Reg\")\n",
    "plt.ylabel(\"Val Loss\")\n",
    "plt.xticks([x for x in np.arange(0, 1.1, 0.1)])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bf7dd-471a-4e39-95a2-81a8752b932e",
   "metadata": {},
   "source": [
    "Para análise, vamos também verificar como o valor dos pesos se alteram em função de lambda. Como sabemos da regularização L2, o valor dos pesos deve, em geral, diminuir na medida em que lambda aumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97642a8a-6aa5-405f-8239-2516afe9b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    weights_i = []\n",
    "    for key, model in models.items():\n",
    "        weights_i.append(model.get_parameters().ws[i])\n",
    "    weights.append(weights_i)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    plt.plot(lambdas, weights[i])\n",
    "plt.title('Pesos x Lambda')\n",
    "plt.xlabel('Lambda Reg')\n",
    "plt.ylabel('Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f8910-f22b-43ab-b777-da3779f4063a",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53744ac5-13f9-4c1d-b14c-589dc7fec2e6",
   "metadata": {},
   "source": [
    "Com lambda = 0.1, vamos determinar finalmente o valor da função de custo no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81081b-020b-40e5-a3f9-1d4d40155054",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = LinearRegressionParameters()\n",
    "parameters.lambda_reg = lambda_reg\n",
    "parameters.initial_weights = np.zeros(len(data.columns) - 1)\n",
    "parameters.epochs = 50\n",
    "parameters.batch_size = 20\n",
    "\n",
    "selected_model = LinearRegressor(parameters)\n",
    "selected_model.fit(X_train, y_train)\n",
    "loss(selected_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
